{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youngwoo Kim\n",
    "!hdfs dfs -D dfs.replication=1 -cp -f data/*.csv hdfs://nn:9000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328      328      hdfs://nn:9000/action_taken.csv\n",
      "317      317      hdfs://nn:9000/agency.csv\n",
      "521.0 K  521.0 K  hdfs://nn:9000/arid2017_to_lei_xref_csv.csv\n",
      "311.6 K  311.6 K  hdfs://nn:9000/counties.csv\n",
      "237      237      hdfs://nn:9000/denial_reason.csv\n",
      "109      109      hdfs://nn:9000/edit_status.csv\n",
      "180      180      hdfs://nn:9000/ethnicity.csv\n",
      "166.8 M  166.8 M  hdfs://nn:9000/hdma-wi-2021.csv\n",
      "41       41       hdfs://nn:9000/hoepa.csv\n",
      "114      114      hdfs://nn:9000/lien_status.csv\n",
      "65       65       hdfs://nn:9000/loan_purpose.csv\n",
      "79       79       hdfs://nn:9000/loan_type.csv\n",
      "129.6 K  129.6 K  hdfs://nn:9000/msamd.csv\n",
      "122      122      hdfs://nn:9000/owner_occupancy.csv\n",
      "92       92       hdfs://nn:9000/preapproval.csv\n",
      "127      127      hdfs://nn:9000/property_type.csv\n",
      "387      387      hdfs://nn:9000/purchaser_type.csv\n",
      "252      252      hdfs://nn:9000/race.csv\n",
      "144      144      hdfs://nn:9000/sex.csv\n",
      "955      955      hdfs://nn:9000/states.csv\n",
      "2.6 M    2.6 M    hdfs://nn:9000/tracts.csv\n",
      "17.2 M   51.6 M   hdfs://nn:9000/user/hive\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -du -h hdfs://nn:9000/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/07 23:52:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "banks_df = spark.read.options(header=True, inferSchema=True).csv(\"hdfs://nn:9000/arid2017_to_lei_xref_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(banks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(respondent_name='First National Bank', arid_2017='110004', lei_2018='5493003EW6T31TGECO83', lei_2019='5493003EW6T31TGECO83', lei_2020='5493003EW6T31TGECO83'),\n",
       " Row(respondent_name='First Mid Bank & Trust, National Association', arid_2017='110045', lei_2018='549300XOTES5TCS8T794', lei_2019='549300XOTES5TCS8T794', lei_2020='549300XOTES5TCS8T794'),\n",
       " Row(respondent_name='First Hope Bank, A National Banking Association', arid_2017='110118', lei_2018='5493003XLOX5FDT9R120', lei_2019='5493003XLOX5FDT9R120', lei_2020='5493003XLOX5FDT9R120')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banks_df.rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "banks_df.rdd.filter(lambda x:\"first\" in x.respondent_name.lower()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "banks_df.filter(lower(banks_df.respondent_name).like(\"%first%\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/07 23:52:46 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/07 23:52:46 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/11/07 23:52:51 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/11/07 23:52:51 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.22.0.5\n",
      "23/11/07 23:52:52 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "23/11/07 23:52:58 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/11/07 23:52:58 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/11/07 23:52:58 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/07 23:52:58 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "banks_df.write.saveAsTable(\"banks\", mode=\"overwrite\")\n",
    "spark.sql(\"\"\"\n",
    "            SELECT * FROM banks\n",
    "            WHERE LOWER(respondent_name) LIKE '%first%'\n",
    "            \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/07 23:53:13 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "loans_df = spark.read.options(header=True, inferSchema=True).csv(\"hdfs://nn:9000/hdma-wi-2021.csv\")\n",
    "loans_df.write.bucketBy(8, 'county_code').saveAsTable(\"loans\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tables = [\"ethnicity\", \"race\", \"sex\", \"states\", \"counties\", \"tracts\", \"action_taken\",\n",
    " \"denial_reason\", \"loan_type\", \"loan_purpose\", \"preapproval\", \"property_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in list_of_tables:\n",
    "    spark.read.options(header=True, inferSchema=True).csv(f\"hdfs://nn:9000/{name}.csv\").createOrReplaceTempView(f\"{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'banks': False,\n",
       " 'loans': False,\n",
       " 'action_taken': True,\n",
       " 'counties': True,\n",
       " 'denial_reason': True,\n",
       " 'ethnicity': True,\n",
       " 'loan_purpose': True,\n",
       " 'loan_type': True,\n",
       " 'preapproval': True,\n",
       " 'property_type': True,\n",
       " 'race': True,\n",
       " 'sex': True,\n",
       " 'states': True,\n",
       " 'tracts': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "tables = spark.sql(\"SHOW TABLES\").toPandas()\n",
    "result = {}\n",
    "for i, row in tables.iterrows():\n",
    "    result[row['tableName']] = row['isTemporary']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19739"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "spark.sql(\"\"\"\n",
    "            SELECT lei_2020, respondent_name FROM banks\n",
    "            INNER JOIN loans ON banks.lei_2020=loans.lei\n",
    "            WHERE respondent_name = 'University of Wisconsin Credit Union'\n",
    "            \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (8)\n",
      "+- Project (7)\n",
      "   +- BroadcastHashJoin Inner BuildLeft (6)\n",
      "      :- BroadcastExchange (3)\n",
      "      :  +- Filter (2)\n",
      "      :     +- Scan parquet spark_catalog.default.banks (1)\n",
      "      +- Filter (5)\n",
      "         +- Scan parquet spark_catalog.default.loans (4)\n",
      "\n",
      "\n",
      "(1) Scan parquet spark_catalog.default.banks\n",
      "Output [2]: [respondent_name#64, lei_2020#68]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [hdfs://nn:9000/user/hive/warehouse/banks]\n",
      "PushedFilters: [IsNotNull(respondent_name), EqualTo(respondent_name,University of Wisconsin Credit Union), IsNotNull(lei_2020)]\n",
      "ReadSchema: struct<respondent_name:string,lei_2020:string>\n",
      "\n",
      "(2) Filter\n",
      "Input [2]: [respondent_name#64, lei_2020#68]\n",
      "Condition : ((isnotnull(respondent_name#64) AND (respondent_name#64 = University of Wisconsin Credit Union)) AND isnotnull(lei_2020#68))\n",
      "\n",
      "(3) BroadcastExchange\n",
      "Input [2]: [respondent_name#64, lei_2020#68]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[1, string, false]),false), [plan_id=614]\n",
      "\n",
      "(4) Scan parquet spark_catalog.default.loans\n",
      "Output [1]: [lei#997]\n",
      "Batched: true\n",
      "Bucketed: false (bucket column(s) not read)\n",
      "Location: InMemoryFileIndex [hdfs://nn:9000/user/hive/warehouse/loans]\n",
      "PushedFilters: [IsNotNull(lei)]\n",
      "ReadSchema: struct<lei:string>\n",
      "\n",
      "(5) Filter\n",
      "Input [1]: [lei#997]\n",
      "Condition : isnotnull(lei#997)\n",
      "\n",
      "(6) BroadcastHashJoin\n",
      "Left keys [1]: [lei_2020#68]\n",
      "Right keys [1]: [lei#997]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(7) Project\n",
      "Output [2]: [lei_2020#68, respondent_name#64]\n",
      "Input [3]: [respondent_name#64, lei_2020#68, lei#997]\n",
      "\n",
      "(8) AdaptiveSparkPlan\n",
      "Output [2]: [lei_2020#68, respondent_name#64]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q6\n",
    "spark.sql(\"\"\"\n",
    "            SELECT lei_2020, respondent_name FROM banks\n",
    "            INNER JOIN loans ON banks.lei_2020=loans.lei\n",
    "            WHERE respondent_name = 'University of Wisconsin Credit Union'\n",
    "            \"\"\").explain(\"formatted\")\n",
    "# respondent_name#64 and lei_2020#68\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
