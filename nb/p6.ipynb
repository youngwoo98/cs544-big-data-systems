{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5c9715-a861-4b13-88f7-3cf7115678e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address     Load       Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  172.30.0.4  86.04 KiB  16      100.0%            270e4f15-9298-4c51-a5e9-9c691b4636f0  rack1\n",
      "UN  172.30.0.2  86.04 KiB  16      100.0%            6db3c7da-aeac-4962-bc6a-3fd38fd159ca  rack1\n",
      "UN  172.30.0.3  86.04 KiB  16      100.0%            6becc145-b33e-4577-a51c-b2848dafbba8  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca525cc-2995-43af-bfd2-7babd31f9aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['p6-db-1', 'p6-db-2', 'p6-db-3'])\n",
    "cass = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d565805-24f2-46cb-a76a-e3839cde0af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fd0f2350fa0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"drop keyspace if exists weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfb84679-c42c-44dd-a9a4-1d5cb3214c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fd0f23515d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"create keyspace weather with replication={'class': 'SimpleStrategy', 'replication_factor': 3};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "618330fe-d96e-4d3c-b531-7e65d2d75f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fd0f351ab60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"use weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0118416-3236-4bbc-bdc1-6562fa5abc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fd12063b490>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"\"\"\n",
    "create type weather.station_record(\n",
    "    tmin INT,\n",
    "    tmax INT\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d017e1d-b2b5-48a2-9c35-2f0c5ff8ed95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fd0f2351270>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"\"\"\n",
    "create table weather.stations(\n",
    "    id  TEXT,\n",
    "    name TEXT static,\n",
    "    date DATE,\n",
    "    record weather.station_record,\n",
    "    PRIMARY KEY (id, date)\n",
    ") WITH CLUSTERING ORDER BY (date ASC)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02ecb160-a454-4ea1-b3ef-e082cd6075f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE weather.stations (\n",
      "    id text,\n",
      "    date date,\n",
      "    name text static,\n",
      "    record station_record,\n",
      "    PRIMARY KEY (id, date)\n",
      ") WITH CLUSTERING ORDER BY (date ASC)\n",
      "    AND additional_write_policy = '99p'\n",
      "    AND bloom_filter_fp_chance = 0.01\n",
      "    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\n",
      "    AND cdc = false\n",
      "    AND comment = ''\n",
      "    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\n",
      "    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n",
      "    AND memtable = 'default'\n",
      "    AND crc_check_chance = 1.0\n",
      "    AND default_time_to_live = 0\n",
      "    AND extensions = {}\n",
      "    AND gc_grace_seconds = 864000\n",
      "    AND max_index_interval = 2048\n",
      "    AND memtable_flush_period_in_ms = 0\n",
      "    AND min_index_interval = 128\n",
      "    AND read_repair = 'BLOCKING'\n",
      "    AND speculative_retry = '99p';\n"
     ]
    }
   ],
   "source": [
    "print(cass.execute(\"describe table weather.stations\").one().create_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64154420-0b56-4c77-a0da-6ae1308b081f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bdb28f4f-2a2a-487e-bdc2-87a0688e5f58;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      ":: resolution report :: resolve 1325ms :: artifacts dl 56ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   0   |   0   |   0   ||   18  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bdb28f4f-2a2a-487e-bdc2-87a0688e5f58\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 18 already retrieved (0kB/35ms)\n",
      "23/11/22 21:48:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"p6\")\n",
    "         .config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.0')\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff356370-c3d4-470a-9861-605b91cbaf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "290ff8e2-f945-4dba-b4a0-5a13c31c7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "df = spark.read.text(\"ghcnd-stations.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "619cfeb8-81e9-41b3-be54-1ad6078f28f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604  17.1167  -61.7833   10.1    ST JO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011647  17.1333  -61.7833   19.2    ST JO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE000041196  25.3330   55.5170   34.0    SHARJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEM00041194  25.2550   55.3640   10.4    DUBAI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEM00041217  24.4330   54.6510   26.8    ABU D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123341</th>\n",
       "      <td>ZI000067969 -21.0500   29.3670  861.0    WEST ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123342</th>\n",
       "      <td>ZI000067975 -20.0670   30.8670 1095.0    MASVI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123343</th>\n",
       "      <td>ZI000067977 -21.0170   31.5830  430.0    BUFFA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123344</th>\n",
       "      <td>ZI000067983 -20.2000   32.6160 1132.0    CHIPI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123345</th>\n",
       "      <td>ZI000067991 -22.2170   30.0000  457.0    BEITB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123346 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    value\n",
       "0       ACW00011604  17.1167  -61.7833   10.1    ST JO...\n",
       "1       ACW00011647  17.1333  -61.7833   19.2    ST JO...\n",
       "2       AE000041196  25.3330   55.5170   34.0    SHARJ...\n",
       "3       AEM00041194  25.2550   55.3640   10.4    DUBAI...\n",
       "4       AEM00041217  24.4330   54.6510   26.8    ABU D...\n",
       "...                                                   ...\n",
       "123341  ZI000067969 -21.0500   29.3670  861.0    WEST ...\n",
       "123342  ZI000067975 -20.0670   30.8670 1095.0    MASVI...\n",
       "123343  ZI000067977 -21.0170   31.5830  430.0    BUFFA...\n",
       "123344  ZI000067983 -20.2000   32.6160 1132.0    CHIPI...\n",
       "123345  ZI000067991 -22.2170   30.0000  457.0    BEITB...\n",
       "\n",
       "[123346 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e381e5b0-aea8-48b8-a6d0-26031df8e0b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td></td>\n",
       "      <td>ST JOHNS COOLIDGE FLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011647</td>\n",
       "      <td></td>\n",
       "      <td>ST JOHNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td></td>\n",
       "      <td>SHARJAH INTER. AIRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEM00041194</td>\n",
       "      <td></td>\n",
       "      <td>DUBAI INTL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEM00041217</td>\n",
       "      <td></td>\n",
       "      <td>ABU DHABI INTL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123341</th>\n",
       "      <td>ZI000067969</td>\n",
       "      <td></td>\n",
       "      <td>WEST NICHOLSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123342</th>\n",
       "      <td>ZI000067975</td>\n",
       "      <td></td>\n",
       "      <td>MASVINGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123343</th>\n",
       "      <td>ZI000067977</td>\n",
       "      <td></td>\n",
       "      <td>BUFFALO RANGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123344</th>\n",
       "      <td>ZI000067983</td>\n",
       "      <td></td>\n",
       "      <td>CHIPINGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123345</th>\n",
       "      <td>ZI000067991</td>\n",
       "      <td></td>\n",
       "      <td>BEITBRIDGE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123346 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id state                            name\n",
       "0       ACW00011604        ST JOHNS COOLIDGE FLD         \n",
       "1       ACW00011647        ST JOHNS                      \n",
       "2       AE000041196        SHARJAH INTER. AIRP           \n",
       "3       AEM00041194        DUBAI INTL                    \n",
       "4       AEM00041217        ABU DHABI INTL                \n",
       "...             ...   ...                             ...\n",
       "123341  ZI000067969        WEST NICHOLSON                \n",
       "123342  ZI000067975        MASVINGO                      \n",
       "123343  ZI000067977        BUFFALO RANGE                 \n",
       "123344  ZI000067983        CHIPINGE                      \n",
       "123345  ZI000067991        BEITBRIDGE                    \n",
       "\n",
       "[123346 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "df = df.select(\n",
    "    expr(\"substring(value, 0, 11)\").alias(\"id\"),\n",
    "    expr(\"substring(value, 39, 2)\").alias(\"state\"),\n",
    "    expr(\"substring(value, 42, 30)\").alias(\"name\"),\n",
    ")\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5861ae5-1925-425c-8b05-f0c5c7065932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, state: string, name: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.where(df.state == 'WI')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a881e0c6-17a3-4b1a-afc6-d3cb3ab01f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "099f5954-c592-47a3-b274-86ff7acf1e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_statement = cass.prepare(\"\"\"\n",
    "INSERT INTO weather.stations (id, name)\n",
    "VALUES (?, ?)\n",
    "\"\"\")\n",
    "\n",
    "for row in data:\n",
    "    cass.execute(insert_statement, (row.id, row.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed1b8496-0da6-4e3f-bd9e-7dae74a2a79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CREATE TABLE weather.stations (\\n    id text,\\n    date date,\\n    name text static,\\n    record station_record,\\n    PRIMARY KEY (id, date)\\n) WITH CLUSTERING ORDER BY (date ASC)\\n    AND additional_write_policy = '99p'\\n    AND bloom_filter_fp_chance = 0.01\\n    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\\n    AND cdc = false\\n    AND comment = ''\\n    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\\n    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\\n    AND memtable = 'default'\\n    AND crc_check_chance = 1.0\\n    AND default_time_to_live = 0\\n    AND extensions = {}\\n    AND gc_grace_seconds = 864000\\n    AND max_index_interval = 2048\\n    AND memtable_flush_period_in_ms = 0\\n    AND min_index_interval = 128\\n    AND read_repair = 'BLOCKING'\\n    AND speculative_retry = '99p';\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "count_stations = cass.execute(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM weather.stations\n",
    "\"\"\")\n",
    "cass.execute(\"describe table weather.stations\").one()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ed4751c-6743-4bfc-bca8-30b91eff4b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MADISON DANE CO RGNL AP       '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "stations = cass.execute(\"\"\"\n",
    "SELECT name\n",
    "FROM weather.stations\n",
    "WHERE id = 'USW00014837'\n",
    "\"\"\")\n",
    "stations.one()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "901d5751-8b15-4e66-b4d8-0d19d05a8e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9014250178872933741"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "stations = cass.execute(\"\"\"\n",
    "SELECT token(id)\n",
    "FROM weather.stations\n",
    "WHERE id = 'USC00470273'\n",
    "\"\"\")\n",
    "stations.one()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9a9bcfa-85ee-4533-9306-2b5f8e4d24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = int(stations.one()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c76eb932-7805-4a44-a5bf-0e8218683e9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "output = subprocess.check_output(['nodetool','ring']).decode('utf-8')\n",
    "lines = output.splitlines()\n",
    "split_lines = [line.split() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb5517fc-50d2-4935-bc54-13056eb484f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for line in split_lines:\n",
    "    for string in line:\n",
    "        if len(string) > 15:\n",
    "            result.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c7ddaee-b9fd-41c2-a1fa-08444daf2fb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#q4\u001b[39;00m\n\u001b[1;32m      2\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[43mresult\u001b[49m[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m<\u001b[39m target:\n\u001b[1;32m      4\u001b[0m     ans \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "#q4\n",
    "ans = 0\n",
    "if int(result[0]) < target:\n",
    "    ans = result[1]\n",
    "else:\n",
    "    i = 1\n",
    "    while target > int(result[i]) and i < len(result):\n",
    "        i += 1\n",
    "        ans = result[i]\n",
    "int(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac4d5a-e21e-4fe2-8193-a096b428264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = cass.execute(\"\"\"\n",
    "SELECT token(id)\n",
    "FROM weather.stations\n",
    "WHERE id = 'USC00470273'\n",
    "\"\"\")\n",
    "stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a36ed-2512-496f-ba03-5a8535bcf4a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cass.execute(\"select record.tmax from weather.stations\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf4812-386a-436f-9991-2a83202dad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not\n",
    "# !unzip records.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d3482-1c9f-4d94-ba8b-4c83a7c7f2ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"p6\").getOrCreate()\n",
    "record_df = spark.read.parquet('records.parquet')\n",
    "record_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198600c1-bdd4-4733-9a26-988420340170",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(record_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b15a680-d61e-4899-bf64-6bef2358bbf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "record_df.rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd987d93-e478-4145-9b6d-a65d253e91d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "records_df = record_df.rdd.filter(lambda x: \"TMAX\" or \"TMIN\" in x.element)\n",
    "# records_df = record_df.rdd.groupBy((\"station\",\"date\"))       \n",
    "records_df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65701fbc-2c2d-46af-a13d-3cb55feb0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "# records_df = record_df.rdd.filter(lambda x: \"TMAX\" or \"TMIN\" in x.element)\n",
    "df_transformed = (record_df\n",
    "                  .filter(record_df.element.isin([\"TMIN\", \"TMAX\"]))\n",
    "                  .groupBy(\"station\", \"date\")\n",
    "                  .pivot(\"element\")\n",
    "                  .agg({'value': 'first'})\n",
    "                  .withColumnRenamed(\"TMAX\", \"tmax\")\n",
    "                  .withColumnRenamed(\"TMIN\", \"tmin\")\n",
    "                  .withColumn(\"date\", to_date(col(\"date\"), \"yyyyMMdd\"))\n",
    "                 )\n",
    "records = df_transformed.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae597634-4a26-46c9-a654-00a9f3bb04f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import grpc\n",
    "import station_pb2_grpc, station_pb2\n",
    "\n",
    "channel = grpc.insecure_channel(\"localhost:5440\")\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "for record in records:\n",
    "    # print(type(record.station),type(record.date), type(record.tmin))\n",
    "    response = stub.RecordTemps(station_pb2.RecordTempsRequest(\n",
    "        station = record.station,\n",
    "        date = str(record.date),\n",
    "        tmin = int(record.tmin),\n",
    "        tmax = int(record.tmax)\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af8f00d-9e31-4bab-8f53-6ebd61f32f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q5\n",
    "response = stub.StationMax(station_pb2.StationMaxRequest(station='USW00014837'))\n",
    "response.tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed14c6-8fe2-4a58-852c-2e4d5e7fad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.format(\"org.apache.spark.sql.cassandra\")\n",
    ".option(\"spark.cassandra.connection.host\", \"p6-db-1,p6-db-2,p6-db-3\")\n",
    ".option(\"keyspace\", \"weather\")\n",
    ".option(\"table\", \"stations\")\n",
    ".load())\n",
    "\n",
    "df.createOrReplaceTempView(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f42e0-9c6c-4bc5-a92c-695ef03873cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q6\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf83ba1-d792-4715-a36b-f56b42d84c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q7\n",
    "avg_diff_df = spark.sql( \"\"\"\n",
    "SELECT id, AVG(record.tmax - record.tmin) as avg_diff\n",
    "FROM stations\n",
    "GROUP BY id\n",
    "HAVING avg_diff is not null\n",
    "\"\"\").toPandas()\n",
    "result = {}\n",
    "for i, row in avg_diff_df.iterrows():\n",
    "    result[row['id']] = row['avg_diff']\n",
    "dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219f805-1b0b-4009-8fb3-ea693b5baeb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#q8\n",
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dad861-84f8-44d3-b350-749597389d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#q9\n",
    "response = stub.StationMax(station_pb2.StationMaxRequest(\n",
    "    station = \"USW00014837\"\n",
    "))\n",
    "response.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a6720-3e77-4033-bc8b-49d93f2490b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q10\n",
    "response = stub.RecordTemps(station_pb2.RecordTempsRequest(\n",
    "    station = \"gas_station\",\n",
    "    date = '2023-11-22',\n",
    "    tmin = 12,\n",
    "    tmax = 34\n",
    "))\n",
    "response.error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
